[
["index.html", "MSc Project Analyses: Projects 1 (colour learning) and 2 (shifting novel object preferences) 1 Background", " MSc Project Analyses: Projects 1 (colour learning) and 2 (shifting novel object preferences) Wyatt Toure Date of this version: 2020-11-19 1 Background Based on previous literature we have good reason to believe two things about guppy exploratory behaviour, defined as the propensity to engage with novelty: that there is a relatively strong environmenetal component to exploratory behaviour (Burns et. al. 2016) that exploratory tendencies across contexts are correlated (De Serrano et al. 2016) My experiments try to examine whether there is a role for experience in the environment in shaping exploratory behaviour and to look at the implications of a potential behavioural corrrelation across novelty contexts. If exploratory tendencies are correlated across contexts because of shared mechanisms producing both behaviours then shifting exploration in one context should also shift exploratory tendencies in another context even without directly training for exploration in that context. To tackle these quesitons I have tried to shift novel object exploration and spatial exploration through reinforcement training. I took a stepwise approach to this. Before I asked whether I could shift novel object preferences I wanted to confirm that: Guppies recognize and respond to novel objects (not discussed here) Guppies can have their preference for objects shifted After confirming those I then felt comfortable to go on and see if I could shift novel object preferences. In this document project 1 corresponds to the experiment where I tried to shift preferences for specific objects and project 2 corresponds to the experiment where I tried to shift preferences for novel objects. "],
["methods.html", "2 Project 1 — General Methods", " 2 Project 1 — General Methods Question: Can I shift the preference for a particular object by reinforcing an object with a food reward. I had one set of guppies trained to the blue object (blue-trained guppies) and another set trained to the green object (green-trained guppies). I performed the following trials: 1 baseline object preference test (Guppy in tank with 1 green and 1 blue object, unrewarded) 20 training trials (Guppy in tank with 1 green and 1 blue object, rewarded for visiting one or the other based on treatment) 1 final object preference test (Guppy in tank with 1 green and 1 blue object, unrewarded) Sample footage of an expeimental trial "],
["model-1-preference-for-the-green-object-at-baseline.html", "3 Model 1 - Preference for the green object at baseline", " 3 Model 1 - Preference for the green object at baseline baseline.data.model = lm(green.object.preference ~ 1, data = baseline.data) At baseline, there was no significant preference for the green object preference across all guppies (p = 0.727). term estimate std.error statistic p.value (Intercept) 1.064679 3.00754 0.3540034 0.727 "],
["model-2-preference-for-the-rewarding-object-during-training.html", "4 Model 2 - Preference for the rewarding object during training", " 4 Model 2 - Preference for the rewarding object during training To see whether fish were responsive during training our second model asks whether the preference for the rewarding object changes throughout training between the treatments. training.data.model = lmer(rewarding.object.preference ~ rewarding.object.colour * trial + (1 | id), data = training.data) term estimate std.error statistic df p.value rewarding.object.colourgreen 55.9885527 27.340786 2.0478033 47.15948 0.046 trial 7.6998004 1.084800 7.0979013 438.05233 &lt; .001 rewarding.object.colourgreen:trial -0.5487883 1.490061 -0.3682991 434.79210 0.713 Throughout training, over the 20 trials, guppies increased their relative preference for the rewarded object by 7.7 seconds each trial (Figure 4.1, p &lt; .001). Figure 4.1: Preference for the green object in both treatments. Negative values represent more time spent with the blue object, positive values indicate more time spent with the green object. Faded lines connect individuals across trials and solid lines represents a linear fit with 95% CI (grey shading). "],
["model-3-preference-for-the-rewarded-object-during-testing-depending-on-treatment.html", "5 Model 3 - Preference for the rewarded object during testing depending on treatment", " 5 Model 3 - Preference for the rewarded object during testing depending on treatment For the main effects of training and rewarding object colour on object preference I fit a linear mixed effects model with fixed effects of trial and rewarding object colour and a random effect of individual id. My third model asks whether the preference for the rewarding object changed between baseline and final test and looks for an interaction with rewarded object colour. In this model trial is a categorical variable composed of the intial preference trial (trial 0) and the final preference trial (trial 21). test.data.model = lmer(rewarding.object.preference ~ rewarding.object.colour * trial + (1 | id), data = test.data) term estimate std.error statistic df p.value rewarding.object.colourgreen 1.644943 16.31541 0.1008214 39.90828 0.92 trial21 17.697361 16.62739 1.0643497 19.99995 0.3 rewarding.object.colourgreen:trial21 65.350597 22.51360 2.9027170 19.99995 0.009 I found a significant interaction effect between trial and rewarding object colour (p = 0.009). Guppies that had green rewarded had a rewarded object preference that was 65.4 seconds stronger than the rewarded object preference of guppies trained to blue (Figure 5.1). Figure 5.1: The initial and final rewarding object preference. Dashed line represents the no preference value. Data are means +/- 95% CI. Bold line connects means across trials. Post-hoc pairwise comparisons investigating the differences between treatments based on whether guppies are untrained or trained reveals that initially there was no difference in the strength of preference for the rewarding object between the treatments (blue-trained guppies had a blue object preference of 1.8 seconds and green-trained guppies had a green object preference of 3.5 seconds, p = 1). Comparing the shift in rewarding object preference between initial and final preference tests in blue-trained and green-trained guppies reveals that the shift in rewarding object preference is significant for green-trained guppies but not for blue-trained guppies. Green trained guppies increased their preference for the green object by 83 seconds (going from a green object preference of 3.5 seconds initially to 86.5 seconds at final test, p &lt; .001) whereas blue-trained guppies non-significantly increased their preference for the blue object by 17.7 seconds (going from a blue object preference 1.8 seconds initially to 19.5 seconds at final test, p = 0.714). For a full description of post-hoc comparisons see table 5.1. Overall I find that: Green-trained guppies increased their preference for the green object. Blue-trained guppies non-significantly increased their preference for the blue object. Table 5.1: Table of post-hoc tests with Tukey adjustment for multiple comparisons. The numbers represent the initial test trial (0) and the final test trial (21). The colour corressponds to the identity of the rewarding object (blue for blue-trained guppies, green for green-trained guppies). Values are all rounded to 3 decimal places. Significant p-values are bolded. contrast estimate df lower.CL upper.CL p.value 0 blue - 21 blue -17.697 20.000 -64.236 28.842 0.714 0 blue - 0 green -1.645 39.908 -45.381 42.092 1 0 blue - 21 green -84.693 39.908 -128.429 -40.956 &lt; .001 21 blue - 0 green 16.052 39.908 -27.684 59.789 0.759 21 blue - 21 green -66.996 39.908 -110.732 -23.259 0.001 0 green - 21 green -83.048 20.000 -125.532 -40.564 &lt; .001 Testing blue-trained guppies separately It does look like blue trained guppies are shifting their preference slightly but the effect might be getting drowned out by the large shift in preference of green-trained guppies so I looked at blue-trained guppies alone in a separate model. blue.guppies.test.data = test.data %&gt;% filter(rewarding.object.colour == &#39;blue&#39;) blue.guppies.test.data.model = lm(rewarding.object.preference ~ trial, data = blue.guppies.test.data) term estimate std.error statistic p.value (Intercept) 1.841809 7.466217 0.2466857 0.808 trial21 17.697361 10.558825 1.6760729 0.111 We see that the shift in preference between baseline trial and the final trial is still not significant (p = 0.111) in blue trained guppies though there seems to be a definite trend and the effect is in the expected direction. The random effect of ID was dropped here since it led to a singular fit, there was much more residual variance compared to individual variance, ID random effect did not significantly improve model fit. "],
["model-4-preference-for-the-rewarded-object-during-training-based-on-feeding.html", "6 Model 4 - Preference for the rewarded object during training based on feeding", " 6 Model 4 - Preference for the rewarded object during training based on feeding It might be the case that some fish behaved in a way that was not conducive to learning the association during training. My fourth model asks whether the time spent near the rewarding object during a training session is influenced by whether a fish ate or not. Here trial is a variable containing the training trials (1-20). It is supplied as a random effect along with individual ID in the model. training.data.model.rewarding.object = lmer(rewarding.object.preference ~ ate + (1 | id) + (1 | trial), data = training.data) term estimate std.error statistic df p.value ateyes 91.18488 10.95274 8.325303 223.7498 &lt; .001 Throughout all of training, fish that ate during a session spent on average 91.2 more seconds near the rewarded object compared to fish that did not eat (Figure, 6.1, p &lt; .001). Fish that spent more trials eating may therefore have received more reinforcement for the object-food association. Figure 6.1: Preference for the rewarding object during training based on whether an individual ate during a trial or not. Dashed line represents the no preference value. Data are means +/- 95% CI. "],
["model-5-is-there-a-difference-in-feeding-attempts-between-treatments.html", "7 Model 5 - Is there a difference in feeding attempts between treatments?", " 7 Model 5 - Is there a difference in feeding attempts between treatments? A discrepancy in reinforcement between treatments may influence performance on a final preference test. To see whether there was a difference in feeding between treatments I counted the number of trials in which an individual fish ate throughout all of training and compared the feeding counts between treatments. To do this I fit a generalized linear model with a Poisson distribution. The response variable 'feeding count' is a sum of the number of trials in which a guppy ate. feeding.data.model = glm(feeding.count ~ rewarding.object.colour, family = &quot;poisson&quot;, data = my.feeding.data) term estimate std.error statistic p.value rewarding.object.colourgreen 0.0709929 0.1264991 0.5612126 0.575 I found no significant difference in the amount of feeding done by individuals trained to green versus individuals trained to blue (Figure 7.1, p = 0.575) which suggests that the observed group-level differences in final test performance between blue-trained guppies versus green-trained guppies cannot be explained by differences in performance during training. Figure 7.1: Average number of trials in which a fish fed during training. Data are means +/- 95% confidence intervals "],
["model-6-what-if-i-control-for-feeding-count.html", "8 Model 6 - What if I control for feeding count?", " 8 Model 6 - What if I control for feeding count? test.data.feeding.controlled.model = lm(rewarding.object.preference ~ trial * rewarding.object.colour + feeding.count, data = test.feeding.data) term estimate std.error statistic p.value trial21 17.6973610 16.880360 1.0483995 0.301 rewarding.object.colourgreen 0.5377365 16.183195 0.0332281 0.974 feeding.count 1.3557636 1.020662 1.3283182 0.192 trial21:rewarding.object.colourgreen 65.3505973 22.856116 2.8592171 0.007 Nothing changes if I control for feeding count. The above model is the feeding controlled model. Below we have the non-feeding-count controlled model from earlier. term estimate std.error statistic df p.value rewarding.object.colourgreen 1.644943 16.31541 0.1008214 39.90828 0.9201970 trial21 17.697361 16.62739 1.0643497 19.99995 0.2998524 rewarding.object.colourgreen:trial21 65.350597 22.51360 2.9027170 19.99995 0.0088005 In both models the p-values are similar. However the effect of feeding count trends in the expected direction in our feeding count controlled model. In the effect plot below to the left we see that as feeding count increases the preference for the rewarding object colour also increases. What if I control for feeding count in just blue guppies. blue.guppies.test.data2 = test.feeding.data %&gt;% filter(rewarding.object.colour == &#39;blue&#39;) blue.guppies.test.data.model2 = lm(rewarding.object.preference ~ trial + feeding.count, data = blue.guppies.test.data2) term estimate std.error statistic p.value (Intercept) -18.679695 12.3858913 -1.508143 0.15 trial21 17.697361 9.7783850 1.809845 0.088 feeding.count 1.848784 0.9257908 1.996978 0.062 The p-values get smaller compared to when we first look at blue-trained guppies separately without controlling for feeding count but there is still no significant effect. Compare the above with the below to see how the values differ. blue.guppies.test.data.model = lm(rewarding.object.preference ~ trial, data = blue.guppies.test.data) term estimate std.error statistic p.value (Intercept) 1.841809 7.466217 0.2466857 0.808 trial21 17.697361 10.558825 1.6760729 0.111 "],
["model-7-does-feeding-count-predict-anything.html", "9 Model 7 - Does feeding count predict anything?", " 9 Model 7 - Does feeding count predict anything? ** These data are just the final preference trial** test.data.feeding.controlled.model1 = lm(rewarding.object.preference ~ feeding.count, data = retest.feeding.data) term estimate std.error statistic p.value feeding.count 2.617822 2.317213 1.129729 0.272 Testing for an effect of feeding count on rewarding object preference finds that there is no significant effect but the effect is in the expected direction. However,there is an effect of feeding on the time spent near both objects at re-test. test.data.feeding.controlled.model2 = lm(time.with.both.objects ~ feeding.count, data = retest.feeding.data) term estimate std.error statistic p.value feeding.count 3.285502 1.180529 2.783076 0.011 During the final test, a fish that had 0 feedings spent 115.299 seconds near both objects whereas fish that fed in 20 trials spent 181.009 seconds near both objects, a 1.6-fold increase. "],
["an-interesting-trend-from-a-very-early-pilot.html", "10 An interesting trend from a very early pilot", " 10 An interesting trend from a very early pilot Objects from a very early experiment pilot.model = lmer(time.with.trained.object ~ trial * object.colour + (1 | id), data = pilot.data) term estimate std.error statistic df p.value trial11:object.colourgreen 24.5018111 12.03267 2.0362733 26.64353 0.052 trial11:object.colourgrey 0.3002667 16.14352 0.0185998 26.64353 0.985 trial11:object.colourorange 12.7124667 16.14352 0.7874654 26.64353 0.438 trial11:object.colourpurple 2.5024000 18.64094 0.1342422 26.64353 0.894 trial11:object.colourred 25.8586333 16.14352 1.6017960 26.64353 0.121 trial11:object.colourwhite 11.6388961 17.66169 0.6589911 30.43282 0.515 trial11:object.colouryellow -1.6348667 13.18113 -0.1240308 26.64353 0.902 ## model: time.with.trained.object ~ trial * object.colour ## ## trial*object.colour effect ## object.colour ## trial blue green grey orange purple red white yellow ## 0 56.22172 47.98031 39.63883 53.18540 46.2453 43.50927 54.78697 64.86343 ## 11 46.11182 62.37222 29.82920 55.78797 38.6378 59.25800 56.31596 53.11867 "],
["project-2-general-methods.html", "11 Project 2 — General methods", " 11 Project 2 — General methods In this experiment I performed the following trials: 2 baseline open field tests (Guppy in empty 5 gallon tank) 3 baseline object exploration tests (Guppy in 5 gallon tank with 1 familiar object and 1 novel object) 2 baseline maze exploration tests (Guppy in 20 gallon tank with 16 compartments) 20 training trials training guppies to novel or familiar objects (Guppy in 5 gallon tank with 1 familiar object and 1 novel object) 3 object exploration re-tests (Guppy in 5 gallon tank with 1 familiar object and 1 novel object) 2 maze exploration re-tests (Guppy in 20 gallon tank with 16 compartments) 1 mate preference test (Female guppy in 5 gallon tank with 1 familiar male and 1 novel male) This analysis so far focuses on the object exploraiton tests and training with the incorporation of spatial exploration and mate preference pending. Sample footage of an expeimental trial Sample trace of an exploratory guppy in a maze trial Sample trace of a non-exploratory guppy in a maze trial "],
["project-2-specific-methods.html", "12 Project 2 - Specific methods", " 12 Project 2 - Specific methods Durring training I had a set of 10 LEGO objects and a set of 20 LEGO objects which served as the novel objects. 20 unique novel objects for 20 individual training trials. Each familiar object had 2 novel objects it was paired to. This was so that I could train the fish asynchronously. There were 10 tanks so each tank experienced 1 pairing and the next day the pairs shifted tanks. I familiarized guppies to their familiar object 24 hours before training sessions by placing a copy of the object in the home tank The during the next days training session a novel object from a set of novel objects was placed in the test tank opposite of a duplicate of the familiar object. Fish were then rewarded for either visiting the familiar object or the novel object. Representative figure of how familiar novel oobject pairs were implemented in the experiment A full trial went like this: A familiar object is placed in the home tank of a group of guppies 24 hours later a training trial beings in a separarte test tank A copy of the familiarr object and its novel object pair are placed opposite from each other in a test tank A guppy is transfered from the home tank to an opaque cup and released into a clear glass cylinder in the center of the 5-gallon test tank The guppy is given 1 minute to habituate to transfer inside the clear glass cylinder The clyinder is removed and the guppy is allowed to move freely about the tank for 10 minutes The guppy is then collected back into the opaque cup and returned to the home tank Sample footage of an expeimental trial "],
["model-1-preference-for-the-novel-object-at-baseline.html", "13 Model 1 - Preference for the novel object at baseline", " 13 Model 1 - Preference for the novel object at baseline baseline.data.model = lmer(novel.object.preference ~ 1 + (1|id) + (1|trial), data = baseline.data) ## boundary (singular) fit: see ?isSingular term estimate std.error statistic df p.value (Intercept) -15.79416 18.90553 -0.8354255 2.000009 0.491 There is no significant preference for the novel object over the familiar object at baseline (p = 0.491). There is a singular fit and closer inspection reveals that the individual ID random effect is not capturing any variance (id standard deviation = 0 whereas the trial standard deviation = 29.531) so I dropped the ID random effect. Values remain unchanged. baseline.data.model.id.dropped = lmer(novel.object.preference ~ 1 + (1|trial), data = baseline.data) term estimate std.error statistic df p.value (Intercept) -15.79416 18.90561 -0.8354223 1.999988 0.491 "],
["baseline-repeatabilities.html", "14 Baseline repeatabilities", " 14 Baseline repeatabilities # Repeatability of time in center of the tank during open field baseline time.in.center.repeatability.open.field = rpt(time.in.center ~ (1 | id), grname = &quot;id&quot;, datatype = &quot;Gaussian&quot;, data = open.field.data, nboot = 50 , npermut = 0) During the first two trials, the open field baselines, time in center is highly repeatable (R = 0.651, p &lt; 0.001). # Repeatability of activity during open field baseline activity.repeatability.open.field = rpt(distance.moved ~ (1 | id), grname = &quot;id&quot;, datatype = &quot;Gaussian&quot;, data = open.field.data, nboot = 50 , npermut = 0) During the open field baselines general activity is not repeatable, (R = 0.253, p = 0.118) # Repeatability of activity during novel object exploration baseline activity.repeatability = rpt(distance.moved ~ (1 | id), grname = &quot;id&quot;, datatype = &quot;Gaussian&quot;, data = baseline.data, nboot = 50 , npermut = 0) General activity was repeatable during the novel object baseline assays R = 0.464 (95% CI = 0.244, 0.711, p = 0). # Repeatability of time spent near novel object at baseline novel.object.pref.rpt = rpt(time.with.novel.object ~ (1 | id), grname = &quot;id&quot;, datatype = &quot;Gaussian&quot;, data = baseline.data, nboot = 50 , npermut = 0) The time spent near a novel object is not repeatable R = 0.122(95% CI = 0, 0.347, p = 0.19) and the novel object preference metric (time near novel object subtracted by time near familiar object) is also not repeatable. # Repeatability of time spent near both objects at baseline time.near.both.objects.rpt = rpt(time.with.both.objects ~ (1 | id), grname = &quot;id&quot;, datatype = &quot;Gaussian&quot;, data = baseline.data, nboot = 50 , npermut = 0) However, the time spent near both objects is significantly repeatable R = 0.223(95% CI = 0.04, 0.451, p = 0.047). "],
["model-2-preference-for-the-rewarded-object-during-training.html", "15 Model 2 - Preference for the rewarded object during training", " 15 Model 2 - Preference for the rewarded object during training To see whether fish were responsive during training our second model asks whether the preference for the rewarding object changes throughout training between the treatments. training.data.model2 &lt;- lmer(rewarding.object.preference ~ treatment * trial + (1|id), data = training.data) term estimate std.error statistic df p.value treatmentnovel 8.9738835 62.356989 0.1439114 34.34104 0.886 trial 6.2096176 1.724413 3.6010031 397.21092 &lt; .001 treatmentnovel:trial 0.4166683 2.683552 0.1552675 398.92605 0.877 Throughout training, over the 20 trials, guppies increased their relative preference for the rewarded object by 6.21 seconds each trial (Figure 15.1, p &lt; .001) with no effect of treatment on performance during training (p = 0.886) suggesting that whether a guppy was trained to familiar objects or novel objects did not influence how individuals behaved towards the rewarding object throughout training. Figure 15.1: Preference for the novel object in both treatments. Negative values represent more time spent with the familiar object, positive values indicate more time spent with the novel object. Faded lines connect individuals across trials and solid lines represents a linear fit with 95% CI (grey shading). "],
["model-3a-preference-for-the-novel-during-testing-based-on-treatment.html", "16 Model 3a - Preference for the novel during testing based on treatment", " 16 Model 3a - Preference for the novel during testing based on treatment For the main effects of training and treatment on novel object preference we fit a linear mixed model with fixed effects of trial (baseline test vs final test) and rewarding object treatment (novelty-rewarded vs familiar-rewarded). test.data = test.data %&gt;% filter(treatment != &#39;control&#39;) test.data.model3a = lmer(novel.object.preference ~ trial.type * treatment + (1 | id) + (1 | trial), data = test.data) ## boundary (singular) fit: see ?isSingular term estimate std.error statistic df p.value trial.typere-test 9.303217 16.33241 0.5696168 128 0.57 treatmentnovel -2.986657 16.71399 -0.1786920 128 0.858 trial.typere-test:treatmentnovel 37.799683 25.02258 1.5106229 128 0.133 When looking at the novel object preference we find no significant effects of trial type or treatment, and find no interaction effect. There is a non-significant trend for an interaction effect of trial type and tretment whereby guppies that were in the novelty-rewarded treatment had a novel object preference that was 37.8 seconds stronger than the novel object preference of guppies in the familiar-rewarded treatment (p = 0.133). Figure 16.1: Plot when response is coded as novel object preference. Data are means +/- 95% CI. "],
["model-3b-preference-for-the-rewarding-object-during-testing-based-on-treatment.html", "17 Model 3b - Preference for the rewarding object during testing based on treatment", " 17 Model 3b - Preference for the rewarding object during testing based on treatment However, I also ran a model where the response variable is coded as the rewarding object preference rather than the novel object preference and this produces different results. test.data.model3b = lmer(rewarding.object.preference ~ trial.type * treatment + (1 | id) + (1 | trial), data = test.data) ## boundary (singular) fit: see ?isSingular term estimate std.error statistic df p.value trial.typere-test -9.303217 16.33241 -0.5696168 128 0.57 treatmentnovel -31.837217 16.71399 -1.9048240 128 0.059 trial.typere-test:treatmentnovel 56.406117 25.02258 2.2542086 128 0.026 When we look at the response variable as the rewarding object preference (i.e. the novel object preference for novelty-trained guppies and the familiar object preeference for familiar-trained guppies), we find that there is a significant interaction effect between trial type and treatment where novelty-trained guppies had a rewarding object preference that was 56.4 seconds stronger than that of familiar-trained guppies (p = 0.026). Figure 17.1: Plot when response is coded as rewarding object preference. Data are means +/- 95% CI. It's hard to make out why this is the case. Look at figures 2 and 3 which show the data presented with the different response variables doesn't make it immediately clear why this should be the case. However, doing post hoc contrasts for both models (Tables 1 and 2) reveal that the main contrasts we are interested in are the same between both models, there is a trend for novelty-trained guppies to increase their novel object preference (p = 0.07 and same effect size in both models). Table 17.1: Pairwise contrasts when response variable is coded as rewarding object contrast estimate df lower.CL upper.CL p.value baseline familiar - (re-test familiar) 9.303 109.258 -33.362 51.969 0.941 baseline familiar - baseline novel 31.837 65.377 -12.227 75.901 0.236 baseline familiar - (re-test novel) -15.266 65.377 -63.855 33.324 0.841 (re-test familiar) - baseline novel 22.534 65.377 -22.416 67.484 0.553 (re-test familiar) - (re-test novel) -24.569 65.377 -73.963 24.826 0.559 baseline novel - (re-test novel) -47.103 119.922 -96.741 2.535 0.07 Table 17.2: Pairwise contrasts when response variable is coded as novel object contrast estimate df lower.CL upper.CL p.value baseline familiar - (re-test familiar) -9.303 109.258 -51.969 33.362 0.941 baseline familiar - baseline novel 2.987 65.377 -41.077 47.051 0.998 baseline familiar - (re-test novel) -44.116 65.377 -92.706 4.474 0.088 (re-test familiar) - baseline novel 12.290 65.377 -32.660 57.240 0.888 (re-test familiar) - (re-test novel) -34.813 65.377 -84.207 14.581 0.256 baseline novel - (re-test novel) -47.103 119.922 -96.741 2.535 0.07 When doing pairwise comparisons, if you do not do all of them it lowers the p-value since it is less tests. But it is not clear whether I should do all the pairwise comparisons or just the ones I knew I wanted to do from the start. "],
["re-test-repeatabilities.html", "18 Re-test repeatabilities", " 18 Re-test repeatabilities # Repeatability of activity at re-test activity.repeatability.retest = rpt(distance.moved ~ (1 | id), grname = &quot;id&quot;, datatype = &quot;Gaussian&quot;, data = retest.data, nboot = 50 , npermut = 0) General activity becomes more repeatable at re-test R = 0.425 (95% CI = 0.135, 0.665, p = 0.001) # Repeatability of novel object preference at re-test novel.object.pref.rpt.retest = rpt(time.with.novel.object ~ (1 | id), grname = &quot;id&quot;, datatype = &quot;Gaussian&quot;, data = retest.data, nboot = 50 , npermut = 0) At re-test the time with spent near the novel object is repeatable R = 0.375 (95% CI = 0.026, 0.617, p = 0.004). It is interesting to note that time spent near the novel object is not rerpeatable at baseline but training induces consistency in time spent near a novel object. # Repeatability of time spent near both objects at re-test time.with.both.objects.rpt.retest = rpt(time.with.both.objects ~ (1 | id), grname = &quot;id&quot;, datatype = &quot;Gaussian&quot;, data = retest.data, nboot = 50 , npermut = 0) Moreover, the time spent near both objects becomes highly repeatable at re-test R = 0.536 (95% CI = 0.223, 0.715, p = 0). However, the novel object preference metric (time spent near novel object subtracted by time spent near familiar object) is still not repeatable at re-test. "],
["model-4-preference-for-the-rewarding-object-during-training-based-on-feeding.html", "19 Model 4 - Preference for the rewarding object during training based on feeding", " 19 Model 4 - Preference for the rewarding object during training based on feeding Our fourth model asks whether the time spent near the rewarding object during a training session is influenced by whether a fish ate or not. training.data.model.rewarding.object4 = lmer(rewarding.object.preference ~ ate + (1 | id) + (1| trial), data = training.data) term estimate std.error statistic df p.value ateY 164.8939 18.90312 8.723104 362.6141 &lt; .001 Throughout all of training, fish that ate during a session spent on average 164.9 more seconds near the rewarded object compared to fish that did not eat (Figure, 6.1, p &lt; .001). Figure 19.1: Preference for the rewarding object during training based on whether an individual ate during a trial or not. Dashed line represents the no preference value. Data are means +/- 95% CI. This is consistent with the results of project 2 where I tried to shift coloured object preferences. In that experiment trials lasted 5 minutes and fish that ate spent 91.2 seconds more near the rewarded object than fish that did not eat. Trials in this experiment lasted 10 minutes and we see here that with a 2x trial length fish spent 1x the amount of time near the rewarded object. "],
["model-5-do-treatments-differ-in-feeding-activity.html", "20 Model 5 - Do treatments differ in feeding activity?", " 20 Model 5 - Do treatments differ in feeding activity? A discrepancy in reinforcement between treatments may influence performance on a final preference test. To see whether there was a difference in feeding between treatments I counted the number of trials in which an individual fish ate throughout all of training and compared the feeding counts between treatments. To do this I fit a generalized linear model with a Poisson distribution. feeding.data.model5 = glm(feeding.count ~ treatment, family = &quot;poisson&quot;, data = my.feeding.data) The response variable 'feeding count' is a sum of the number of trials in which a guppy ate and the fixed effect 'object' represents the rewarding object identity. term estimate std.error statistic p.value treatmentnovel -0.0617256 0.1351969 -0.4565605 0.648 I find no significant difference in the amount of feeding done by individuals trained to green versus individuals trained to blue (Figure 20.1, p = 0.648) which suggests that the observed group-level differences in final test performance between blue-trained guppies versus green-trained guppies cannot be explained by differences in performance during training. Figure 20.1: Average number of trials in which a fish fed during training "],
["controlling-for-feeding-count.html", "21 Controlling for feeding count", " 21 Controlling for feeding count Rewarding object preference test.data.feeding.controlled.model6 = lmer(rewarding.object.preference ~ trial.type * treatment + feeding.count + (1|id) + (1|trial), data = test.feeding.data) ## boundary (singular) fit: see ?isSingular term estimate std.error statistic df p.value trial.typere-test -8.781274 16.2684917 -0.5397718 127 0.59 treatmentnovel -27.236156 16.9485937 -1.6069862 127 0.111 feeding.count 1.313277 0.9123614 1.4394258 127 0.152 trial.typere-test:treatmentnovel 52.735295 25.0486140 2.1053179 127 0.037 Testing for an effect of feeding count on rewarding object preference finds that there is no significant effect but the effect is in the expected direction and including feeding count as a covariate in model 3b does not change the results. There is an effect of feeding on the time spent near both objects. The following are the effect predictions of the model. ## model: time.with.both.objects ~ feeding.count ## ## feeding.count effect ## feeding.count ## 0 4.8 9.5 14 19 ## 48.13972 71.52923 94.43145 116.35912 140.72319 There is a 2.9-fold increase in the time spent near both objects from 0 feedings to 19 feedings as predicted by the model. This result is highly consistent with the one seen in the previous colour learning experiment. A significant effect of number of trials fed on time spent near both objects and a non-significant trend for feeding count to increase the preference for the rewarding object. Given that the time spent near both objects is highly repeatable and that the time spent near both objects is influenced by feeding during training and that there is no effect of treatment (but there is a trend, see models 3a and 3b) it seems that the guppies are becoming responsive to training but not discriminating the objects they spend time with, choosing instead to spend more time with all objects regardless of identity. "],
["why-so-little-power.html", "22 Why so little power", " 22 Why so little power Figure 22.1: Time near both objects at re-test for females and males of various treatments "]
]
