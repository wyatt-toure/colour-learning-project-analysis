---
title: "Analysis for 'Colour biases in learned foraging preferences in Trinidadian guppies'"
output:
  bookdown::html_document2:
    include:
      highlight: pygments
    css: styles.css
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: false
      smooth_scroll: true
    number_sections: false
    split_by: section
    
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_file = 'docs/analysis')
  })
---

<div class="topnav">
  <a href="index.html">Guppy colour learning project site</a>
    <a href="https://github.com/wyatt-toure/guppy-colour-learning-project" style = "float: right;">GitHub</a>
    <a class="active" href="analysis.html" style = "float: right;">Analysis</a>
    <a href="methods.html" style = "float: right;">Methods</a>
    <a href="index.html" style = "float: right;">Home</a>
</div>

<p class="author-name">M. Wyatt Toure<span class="affil-mark">1*</span>, Simon M. Reader<span class="affil-mark">1</span></p>

<p class="author-affil"><span class="affil-mark">1</span>McGill University, Department of Biology, 1205 Docteur Penfield, Montreal, Quebec H3A 1B1, Canada</p>
<p><span class="affil-mark">*</span>Corresponding Author: <br>M. Wyatt Toure<br> e-mail: `m_wyatt.toure@mail.mcgill.ca`<br></p>

Date of last update: `r format(Sys.Date(), '%b %d %Y')`

***

## Brief Overview

This page reports the analyses for the experiment described in
<span style="text-decoration:underline">'Colour biases in learned foraging
preferences in Trinidadian guppies'</span>. The raw data used to conduct these
analyses are available in the
[colour-learning-project-data.csv](https://github.com/wyatt-toure/guppy-colour-learning-project/blob/main/data/colour-learning-project-data.csv)
file. Descriptions of the variables found in the dataset are given in the
metadata section of the
[README](https://github.com/wyatt-toure/guppy-colour-learning-project#metadata)
file. The R script to reproduce the analysis and this site are in the
[analysis-script-colour-project.Rmd](https://github.com/wyatt-toure/guppy-colour-learning-project/blob/main/analysis-script-colour-project.Rmd)
file. The methods used to produce this data can be found on the
[Methods](methods.html) page.

***

```{r library-prep, include=FALSE}
# Loading required packages
library(lme4)
library(tidyr)
library(lmerTest)
library(ggplot2)
library(ggpubr)
library(DHARMa)
library(dplyr)
library(effects)
library(broom)
library(broom.mixed)
library(knitr)
library(emmeans)
library(report)
library(cowplot)
library(tidyext)
library(glmmTMB)
library(MASS)
source("R/format-p-value.R")
source("R/rename-lme4-model.R")
source("R/geom-flat-violin.R")
```

```{r data-prep, include=FALSE}
# Reading in Data
my_data <- read.csv("data/colour-learning-project-data.csv")

# Creating new variables

## Rewarding object preference
my_data <- my_data %>%
  group_by(id) %>%
  mutate(rewarding.object.preference = (time.with.trained.object - time.with.untrained.object))

## Green object preference
my_data <- my_data %>%
  mutate(
    green.object.preference =
      case_when(
        rewarding.object.colour == "green" ~ (time.with.trained.object - time.with.untrained.object),
        rewarding.object.colour == "blue" ~ (time.with.untrained.object - time.with.trained.object)
      )
  )

# Restrict data to training data
training_data <- my_data %>%
  filter(trial.type == "training")

# Restrict data to only the baseline and re-test data
test_data <- my_data %>%
  filter(trial.type == "test")

# Restrict data to only the baseline data
baseline_data <- my_data %>%
  filter(trial == 0)

# Change trial to factor
test_data$trial <- as.factor(test_data$trial)
baseline_data$trial <- as.factor(baseline_data$trial)
```

## Models

We analysed the data from our experiment using linear mixed effect and
generalized linear mixed effect models with the `lmer()` and `glmer()` functions
from the `lme4` package. P-values and effective degrees of freedom were obtained
using the `lmerTest` package. Model residuals were checked they met
distributional assumptions with the `DHARMa` package, you can click the 'See
Model Residuals' button below the model formulas to see the residual diagnostic
plots produced by `DHARMa` for that particular model.

</br>

### Model 1 -  Preference for the green object at baseline

This first model contains the data for all individual guppies. I looked at the
green object preference of all guppies in an intercept only model to see if the
green object preference at baseline was significantly different from zero.
`green.object.preference` is the time spent near the green object subtracted by
the time spent near the blue object

```{r model-1, echo=TRUE}
baseline_data_model <-
  lm(green.object.preference ~ 1,
    data = baseline_data
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName"> See Model 1 Residuals </button>  
<div id="BlockName" class="collapse"> 

```{r, echo=FALSE, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = baseline_data_model)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "model-1-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/residual-plots/",
  device = "png",
  dpi = 300
)
```

</div>

</br>

##### Result

```{r tidying-model-1, echo=FALSE, message=FALSE}
# Setting table row names
baseline_table_row_name_vec <- c("Intercept")

# Converting data frame to tibble
tidy_baseline_model <- broom.mixed::tidy(baseline_data_model)

# Changing tibble header names
tidy_baseline_model <- rename_tidy_lme4_cols(tidy_baseline_model)

# Changing tibble row names
tidy_baseline_model[1:1, 1] <- baseline_table_row_name_vec
```

```{r,  results=TRUE, echo=FALSE}
knitr::kable(tidy_baseline_model %>%
  mutate_if(is.numeric, round, digits = 3))
```

During the initial test, there was no significant preference for the green
object across all guppies (p = `r tidy_baseline_model$'P value' %>% round(3)`).


```{r baseline-pref-plot, echo=FALSE, message=FALSE, fig.cap="Preference for the green object relative to the blue object across all guppies at baseline. Negative values represent more time spent with the blue object, positive values indicate more time spent with the green object. Data are means +/- 95% CI", fig.id="baseline-pref-plot",  warning=FALSE, message=FALSE}
###### Baseline green object preference plot ######
baseline_data_x_axis_label <- "Initial Test"
ggplot(
  baseline_data,
  aes(
    x = trial,
    y = green.object.preference
  )
) +
  theme_classic() +
  ylab("Green object preference (sec)") +
  xlab("") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5)
  ) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_jitter(width = 0.04, alpha = 0.3) +
  stat_summary(
    geom = "point",
    fun = "mean",
    size = 4.5,
    shape = 15
  ) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci", position = position_dodge(width = 0), width = 0.1
  ) +
  scale_x_discrete(labels = baseline_data_x_axis_label)

ggsave(
  filename = "model-1-baseline-data-plot.png",
  path = "figs/",
  device = "png",
  dpi = 300
)
```

***

### Model 2 -  Preference for the rewarding object during training

To see whether fish were responsive during training our second model asks
whether the preference for the rewarding object changes throughout training and
whether the change in rewarding object preference is different between the
treatments.

  - **Response variable:** `rewarding.object.preference` is the time (seconds)
    spent near the rewarding object subtracted by the time spent near the
    unrewarding object
  - **Fixed effect:** `rewarding.object.colour` is the identity of the rewarding
    object (blue or green)
  - **Fixed effect:** `trial` is the number of the training trial. In this model
    it is supplied as an integer
  - **Random effect:** `id` is the identity of the individual fish

```{r model-2, echo=TRUE}
training_data_model <-
  lmer(rewarding.object.preference ~ rewarding.object.colour * trial + (1 | id),
    data = training_data
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName2"> See Model 2 Residuals </button>  
<div id="BlockName2" class="collapse">  

```{r, echo=FALSE, message=FALSE}
# Residual diagnostics
simulationOutput <- simulateResiduals(
  fittedModel = training_data_model,
  n = 1000
)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "model-2-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/residual-plots/",
  device = "png",
  dpi = 300
)
```

There is a slight deviation in the upper quantile but no indication in the
residual plot of a gross model misfit.

</div>

```{r tidying-model-2, echo=FALSE, message=FALSE}
# Setting table row names
training_model_table_row_name_vec <- c(
  "Intercept",
  "Reward object colour",
  "Trial",
  "Rewarding object colour X Trial"
)

# Converting data frame to tibble
tidy_training_data_model <- broom.mixed::tidy(training_data_model)

# Formatting p value
tidy_training_data_model$p.value <- format_p_value(tidy_training_data_model$p.value)

# Getting model confidence intervals
training_data_model_confint <- tibble::as_tibble((training_data_model %>%
  confint()), rownames = "factor")

# Changing tibble header names
tidy_training_data_model <- rename_tidy_lme4_cols(tidy_training_data_model)

# Changing tibble row names
tidy_training_data_model[1:4, 3] <- training_model_table_row_name_vec
```

</br>

##### Results

```{r,  results=TRUE, echo=FALSE}
knitr::kable(tidy_training_data_model[1:4, ] %>%
  dplyr::select(-group, -effect) %>%
  mutate_if(is.numeric, round, digits = 3))
```

There was a significant effect of trial. Over the 20 training trials, guppies in
the two treatments increased their relative preference for their respective
rewarded objects by `r tidy_training_data_model$Estimate[3] %>% round(3)`
seconds each trial (Figure \@ref(fig:colour-pref-training-plot), p
`r tidy_training_data_model$'P value'[3]`). There was also a significant effect
of rewarded-object colour (p = `r tidy_training_data_model$'P value'[2]`).
during training green-rewarded guppies expressed a stronger preference for their
rewarded object (the green object) than did blue-rewarded guppies did for the
blue object. However, there was no significant interaction effect between
rewarding object colour and trial (p =
`r tidy_training_data_model$'P value'[4]`), *i.e.*, the rate of increase in
object preference over trials did not significantly differ between the
treatments.

```{r colour-pref-training-plot, echo=FALSE, message=FALSE, fig.cap="Relative preference for the green object in both treatments during training trials (trials 1-20). Negative values represent more time spent with the blue object, positive values indicate more time spent with the green object. Light lines connect individuals across trials and bold lines represents a linear fit with 95% CI (grey shading). Subjects were consistently rewarded for approaching the blue object (dashed lines) or the green object (solid lines).", fig.id="colour-pref-training-plot",  warning=FALSE, message=FALSE}
###### Time with rewarding object plot during training ######
ggplot(
  training_data,
  aes(
    x = trial,
    y = green.object.preference,
    color = rewarding.object.colour,
    linetype = rewarding.object.colour
  )
) +
  theme_cowplot() +
  ylab("Green object preference (sec)") +
  xlab("Trial") +
  labs(col = "Rewarding object colour") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5)
  ) +
  scale_color_manual(values = c("#2980b9", "#27ae60")) +
  scale_linetype_manual(values = c("longdash", "solid")) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_line(aes(group = id), alpha = 0.25) +
  geom_smooth(method = lm)

ggsave(
  filename = "model-2-colour-pref-training-plot.png",
  path = "figs/",
  device = "png",
  dpi = 300
)
```

***

 

### Model 3 -  Preference for the rewarded object during testing depending on treatment {#model-3}

For the main effects of training and rewarding object colour on rewarding object
preference I fit a generalized linear mixed effects model with a Gaussian
distribution which modeled the variances to account for variance heterogeneity
using the package
[`glmmTMB`](https://cran.r-project.org/web/packages/glmmTMB/index.html). My
third model asks whether the preference for the rewarding object changed between
baseline and final test and looks for an interaction with rewarded object
colour.

  - **Response variable:** `rewarding.object.preference` is the time (seconds)
    spent near the rewarding object subtracted by the time spent near the
    unrewarding object
  - **Fixed effect:** `rewarding.object.colour` is the identity of the rewarding
    object (blue or green)
  - **Fixed effect:** `trial` is the number of the training trial. In this model
    it is supplied as a factor where 0 is the baseline trial and 21 is the final
    trial
  - **Random effect:** `id` is the identity of the individual fish

```{r model-3, echo=TRUE}
test_data_model_glm = 
  glmmTMB(rewarding.object.preference ~  
            trial * rewarding.object.colour + (1|id) + 
            diag(0 + rewarding.object.colour:trial |id), 
  data = test_data, 
  family = gaussian
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName3"> See Model 3 Residuals </button>  
<div id="BlockName3" class="collapse">  

```{r, include=TRUE,echo=FALSE, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = test_data_model_glm, n = 1000)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "model-3-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/residual-plots/",
  device = "png",
  dpi = 300
)
```

</div>



```{r tidying-model-3, echo=FALSE, message=FALSE}
# Setting table row names
test_model_table_row_name_vec <- c(
  "Intercept",
  "Trial",
  "Rewarding object colour",
  "Rewarding object colour X Trial"
)

# Converting data frame to tibble
tidy_test_data_model <- broom.mixed::tidy(test_data_model_glm)

# Formatting p value
tidy_test_data_model$p.value <- format_p_value(tidy_test_data_model$p.value)

# Getting model confidence intervals
# test.data.model.confint = tibble::as_tibble((test.data.model %>% confint()), rownames = "factor")

# Changing tibble header names
tidy_test_data_model <- rename_tidy_lme4_cols(tidy_test_data_model)

# Changing tibble row names
tidy_test_data_model[1:4, 4] <- test_model_table_row_name_vec
```

</br>

##### Results

```{r, results=TRUE, echo=FALSE}
knitr::kable(tidy_test_data_model[1:4, ] %>%
  dplyr::select(-group, -effect, -component) %>%
  mutate_if(is.numeric, round, digits = 3),
caption = "Summary table for Model 3. Estimates ± standard error (SE) of the 
effects of trial and rewarding object colour on the rewarding object preference 
from the generalized linear mixed effect model containing the effects Trial, 
Rewarding object colour, and their interaction effect (Trial X Rewarding object 
colour)."
)
```

When comparing the initial and final preference test, both conducted after
training and without food rewards present, we found a significant interaction
effect between test and rewarding object colour (p =
`r tidy_test_data_model$'P value'[4]`). Guppies that had been green-rewarded had
a shift in their rewarded object preference that was on average
`r tidy_test_data_model$Estimate[4] %>% round(1)` seconds stronger than the
shift in rewarded object preference of guppies trained to blue (Figure
\@ref(fig:test-data-pref-plot)). These results were unaffected by the removal of
one fish that did not feed during training ([See ESM model 2](#esm-model-2)).

```{r test-data-pref-plot, echo=FALSE, message=FALSE, fig.cap="Changes in object preference from an initial test before training to a final test after training. During training, fish were rewarded for approaching the blue object (blue squares and lines) or the green object (green squares and lines). At test, no food reward was present. Dashed line represents an equal preference for either object. Data are means ± 95% CI; lighter points and lines are data for each individual. The final preference for green-rewarded fish is stronger than that of fish that has been rewarded for approaching the blue object, but both sets of subjects increased their preference for the rewarded object compared to the initial test preference.", fig.id="test-data-pref-plot", warning=FALSE, message=FALSE}
###### Time with rewarding object plot during testing ######
testing_data_x_axis_labels <- c("Initial test", "Final test")
object_labels <- c("Blue-trained", "Green-trained")
names(object_labels) <- c("blue", "green")

ggplot(
  test_data,
  aes(
    x = trial,
    y = rewarding.object.preference,
    color = rewarding.object.colour,
    shape = rewarding.object.colour
  )
) +
  theme_cowplot() +
  geom_jitter(width = 0, alpha = 0.3) +
  stat_summary(geom = "point", fun = "mean", size = 4.5) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci",
    position = position_dodge(width = 0),
    width = 0.1
  ) +
  ylab("Rewarding object preference (sec)") +
  xlab("Trial") +
  labs(col = "Rewarding object colour") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5)
  ) +
  scale_color_manual(values = c("#2980b9", "#27ae60")) +
  scale_shape_manual(values = c(15, 16)) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.6) +
  geom_line(aes(group = id), alpha = 0.2) +
  scale_x_discrete(labels = testing_data_x_axis_labels) +
  facet_grid(~rewarding.object.colour,
    labeller = labeller(rewarding.object.colour = object_labels)
  ) +
  stat_summary(fun = mean, geom = "line", aes(group = rewarding.object.colour))

ggsave(
  filename = "model-3-test-data-pref-plot.png",
  path = "figs/",
  device = "png",
  dpi = 300
)
```

</br>

#### Post-hoc Comparisons

To determine whether the means of the final rewarding object preference for the
two treatments were different I conducted post-hoc comparisons with the package
[`emmeans`](https://cran.r-project.org/package=emmeans). I compared the
following means:

  - Final test blue-trained and initial test blue-trained
  - Final test green-trained and initial test green-trained
  - Final test green-trained and final test blue-trained
  - Initial test green-trained and initial test blue-trained

```{r post-hoc-comparisons, echo=TRUE}
test_data_model_emmeans <- emmeans(test_data_model_glm,
  specs = ~ trial:rewarding.object.colour
)

# Making vectors to represent means of interest from emmeans() output
blue0 <- c(1, 0, 0, 0)
blue21 <- c(0, 1, 0, 0)
green0 <- c(0, 0, 1, 0)
green21 <- c(0, 0, 0, 1)

# Set seed to prevent confidence intervals from changing when code is re-run
set.seed(123)

custom_contrasts <- contrast(test_data_model_emmeans,
  method = list(
    "21 blue - 0 blue" = blue21 - blue0,
    "21 green - 0 green " = green21 - green0,
    "21 green - 21 blue" = green21 - blue21,
    "0 green - 0 blue" = green0 - blue0
  ), adjust = "mvt"
) %>%
  summary(infer = TRUE)
```

</br>

##### Results

```{r contrasts-table-formatting, echo=FALSE, fig.id="contrasts-table", warning=FALSE}
# Convert contrasts to data frame
pairwise_comparison_contrasts_table <- as.data.frame(custom_contrasts)

# Formatting p values
pairwise_comparison_contrasts_table$p.value <- format_p_value(pairwise_comparison_contrasts_table$p.value)
```

```{r contrasts-table-display, echo=FALSE}
knitr::kable(pairwise_comparison_contrasts_table %>%
  dplyr::select(contrast, estimate, lower.CL, upper.CL, df, p.value) %>%
  mutate_if(is.numeric, round, digits = 3) %>%
  rename(
    "Contrast" = contrast,
    "Estimate" = estimate,
    "Lower CL" = lower.CL,
    "Upper CL" = upper.CL,
    "P Value" = p.value
  ),
caption = "Table of post-hoc tests with a multivariate-t adjustment for multiple 
comparisons of a selected set of means. The numbers represent the initial test 
(0) and the final test (21). The colour corresponds to the identity of the 
rewarding object (blue for blue-rewarded guppies, green for green-rewarded 
guppies). Values are all rounded to 3 decimal places. CL = confidence limit."
)
```

Post-hoc comparisons (Table \@ref(tab:contrasts-table-display)) reveal that
initially, before training, there was no significant difference in the strength
of preference for the rewarded object between the treatments (p =
`r pairwise_comparison_contrasts_table$p.value[4]`). The shift in rewarded
object preference between the initial and final preference tests was significant
for green-trained guppies but not for blue-trained guppies: green trained
guppies increased their preference for the green object by
`r abs(pairwise_comparison_contrasts_table$estimate[2] %>% round(1))` seconds
from initial to final (p `r pairwise_comparison_contrasts_table$p.value[2]`)
test whereas blue-trained guppies increased their preference for the blue object
by `r abs(pairwise_comparison_contrasts_table$estimate[1] %>% round(1))` seconds
an effect that was not statistically significant (p =
`r pairwise_comparison_contrasts_table$p.value[1]`). At final test,
green-rewarded guppies had a significantly stronger preference for the
previously rewarded object compared to the blue-rewarded guppies (p =
`r pairwise_comparison_contrasts_table$p.value[3]`).

***

```{r feeding-data-prep, include = FALSE}
#### Get feeding data #####
# Group by ID and count the number of sessions in which an individual ate
feeding <- my_data %>%
  group_by(id) %>%
  count(feeding.count = ate == "yes")

# Remove NAs from this
feeding <- na.omit(feeding)

# Count only the yeses
feeding <- feeding %>%
  filter(feeding.count == "TRUE")

# Remove the column feeding.count to keep only the counts
feeding <- feeding %>%
  dplyr::select(-feeding.count)

# Add the feeding values to the main data frame so I can get treatment IDs
my_feeding_data <- left_join(baseline_data, feeding, by = "id")

# Replace NAs with 0, rename n to feeding count, and extract id, feeding count,
# and rewarding object colour treatment
my_feeding_data <- my_feeding_data %>%
  replace_na(list(n = 0)) %>%
  rename(feeding.count = n) %>%
  dplyr::select(id, feeding.count, rewarding.object.colour)
```

### Model 4 - Is there a difference in feeding attempts between treatments? {#model-4}

A discrepancy in reinforcement between treatments may influence performance on a
final preference test. To see whether there was a difference in feeding between
treatments I counted the number of trials in which an individual fish ate
throughout all of training and compared the feeding counts between treatments.
To do this I fit a generalized linear model with a negative binomial
distribution. The response variable 'feeding count' is a sum of the number of
trials in which a guppy ate.

  - **Response variable:** `feeding.count` is the number of trials in which an
    individual fish ate
  - **Fixed effect:** `rewarding.object.colour` is the identity of the rewarding
    object (blue or green)

##### Model

```{r model-4, echo=TRUE}
feeding_data_model <-
  glm.nb(feeding.count ~ rewarding.object.colour,
    data = my_feeding_data
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName4"> See Model 4 Residuals </button>  
<div id="BlockName4" class="collapse">  

```{r, echo=FALSE, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = feeding_data_model)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "model-4-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/residual-plots/",
  device = "png",
  dpi = 300
)
```

</div>

```{r tidying-model-4, echo=FALSE, message=FALSE}
# Setting table row names
feeding_model_table_row_name_vec <- c(
  "Intercept",
  "Rewarding object colour"
)

# Converting data frame to tibble
tidy_feeding_data_model <- broom.mixed::tidy(feeding_data_model)

# Getting model confidence intervals
feeding_data_model_confint <- tibble::as_tibble((feeding_data_model %>%
  confint()), rownames = "factor")

# Changing tibble header names
tidy_feeding_data_model <- rename_tidy_lme4_cols(tidy_feeding_data_model)

# Changing tibble row names
tidy_feeding_data_model[1:2, 1] <- feeding_model_table_row_name_vec
```

</br>

##### Results

```{r, results=TRUE, echo=FALSE}
knitr::kable(tidy_feeding_data_model[1:2, ] %>%
  mutate_if(is.numeric, round, digits = 3))
```

We found no significant difference in the number of trials individuals fed
between green-rewarded and blue-rewarded fish (Figure
\@ref(fig:feeding-count-plot), p =
`r tidy_feeding_data_model$'P value'[2] %>% round(3)`). Removing the one
individual that did not feed during training gave a similar result (p = 0.87).
We also incorporated feeding count as a covariate in [Model 3](#model-3),
finding the same pattern of results (See [ESM Model 2](#esm-model-2)).

```{r feeding-count-plot, echo=FALSE, message=FALSE, fig.cap="Average number of trials in which a fish fed during training. Data are means ± 95% confidence intervals with probability density functions of the data (showing the negative binomial distribution) to the right of the raw data.", fig.id="training-data-ate-plot", warning=FALSE}

ggplot(
  my_feeding_data,
  aes(
    x = rewarding.object.colour,
    y = feeding.count,
    fill = rewarding.object.colour,
    colour = rewarding.object.colour
  )
) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.8) +
  geom_flat_violin(
    aes(fill = rewarding.object.colour),
    position = position_nudge(x = .25, y = 0),
    adjust = 0.7,
    alpha = 0.4,
    trim = FALSE,
    color = NA
  ) +
  stat_summary(geom = "point", fun = "mean", size = 4.5, shape = 15) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci",
    position = position_dodge(width = 0),
    width = 0.1
  ) +
  ylim(-3, 23) +
  ylab("Rewarding object preference") +
  xlab("Trial") +
  theme_cowplot() +
  guides(fill = FALSE, colour = FALSE) +
  ylab("Number of trials fed") +
  xlab("Rewarding object colour") +
  labs(col = "Rewarding object colour") +
  theme(
    legend.position = "top",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5)
  ) +
  scale_color_manual(values = c("#2980b9", "#27ae60")) +
  scale_fill_manual(values = c("#2980b9", "#27ae60"))

# Saving plot
ggsave(
  filename = "model-4-feeding-count-plot.png",
  path = "figs/",
  device = "png",
  dpi = 300
)
```

***


## ESM Models

In this section we describe models not included in the main text.


### Addressing feeding count

The models described in this section were run to address the potential role of
feeding count on test performance. The concern was that the results may be
explained by differential levels of reinforcement between the groups or between
individuals during training. To ensure our results were robust to matters
arising from feeding count variation we:

1)  Removed an individual that did not feed and re-ran the analysis in
    [Model 3](#model-3)
2)  Included feeding count as a co-variate and re-ran the analysis in
    [Model 3](#model-3)

</br>

#### ESM Model 1 - Removing individual that did not feed

```{r, include=FALSE}
test_feeding_data <- left_join(test_data, feeding, by = "id")
test_feeding_data <- test_feeding_data %>% replace_na(list(n = 0))
test_feeding_data <- rename(test_feeding_data, feeding.count = n)
retest_feeding_data <- test_feeding_data %>% filter(trial == "21")
```

There is a fish that did not eat during any trials, however removing this individual does not change the conclusions 

```{r, echo=TRUE}
test_feeding_data_low_feeders_removed <-
  test_feeding_data %>% filter(feeding.count > 0)
```


##### Model

```{r, echo=TRUE}
test_feeding_data_low_feeders_removed_model <-
  glmmTMB(rewarding.object.preference ~
  trial * rewarding.object.colour + (1 | id) +
    diag(0 + rewarding.object.colour:trial | id),
  data = test_feeding_data_low_feeders_removed,
  family = gaussian
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName6"> See ESM Model 1 Residuals </button>  
<div id="BlockName6" class="collapse">  

```{r, echo=FALSE, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = test_feeding_data_low_feeders_removed_model)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "ESM-model-1-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/residual-plots/",
  device = "png",
  dpi = 300
)
```

</div>

</br>

```{r, echo=FALSE, results=FALSE}
# Setting table row names
feeding_data_low_feeders_removed_model_table_row_name_vec <- c(
  "Intercept",
  "Trial",
  "Rewarding object colour",
  "Rewarding object colour X Trial"
)

tidy_fit_test_feeding_data_low_feeders_removed <- broom.mixed::tidy(test_feeding_data_low_feeders_removed_model)

# format p value
tidy_fit_test_feeding_data_low_feeders_removed$p.value <- format_p_value(tidy_fit_test_feeding_data_low_feeders_removed$p.value)

# Changing tibble header names
tidy_fit_test_feeding_data_low_feeders_removed <- rename_tidy_lme4_cols(tidy_fit_test_feeding_data_low_feeders_removed)

tidy_fit_test_feeding_data_low_feeders_removed[1:4, 4] <- feeding_data_low_feeders_removed_model_table_row_name_vec
```

##### Results

```{r, results=TRUE, echo=FALSE}
knitr::kable(tidy_fit_test_feeding_data_low_feeders_removed[1:4, ] %>%
  dplyr::select(-group, -effect, -component) %>%
  mutate_if(is.numeric, round, digits = 3))
```

***

#### ESM Model 2 - Including feeding count as a covariate {#esm-model-2}

With this next model we looked to see whether including the amount of trials an
individual fed as a covariate in the model changes the conclusions.

  - **Response variable:** `rewarding.object.preference` is the time (seconds)
    spent near the rewarding object subtracted by the time spent near the
    unrewarding object
  - **Fixed effect:** `rewarding.object.colour` is the identity of the rewarding
    object (blue or green)
  - **Fixed effect:** `trial` is the number of the training trial. In this model
    it is supplied as an integer
  - **Random effect:** `id` is the identity of the individual fish
  - **Covariate:** `feeding.count` is the number of trials in which an
    individual fish ate

```{r, echo = TRUE}
test_data_feeding_controlled_model <-
  glmmTMB(rewarding.object.preference ~
  trial * rewarding.object.colour + feeding.count + (1 | id) +
    diag(0 + rewarding.object.colour * trial | id),
  data = test_feeding_data,
  family = gaussian
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName7"> See ESM Model 2 Residuals </button>  
<div id="BlockName7" class="collapse">  

```{r, echo=FALSE, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = test_data_feeding_controlled_model)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "ESM-model-2-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/residual-plots/",
  device = "png",
  dpi = 300
)
```

</div>

</br>
```{r, echo=FALSE, results=FALSE}
# Setting table row names
test_data_feeding_controlled_model_table_row_name_vec <- c(
  "Intercept",
  "Trial",
  "Rewarding object colour",
  "Feeding count",
  "Rewarding object colour X Trial"
)

tidy_fit_test_data_feeding_controlled_model <- broom.mixed::tidy(test_data_feeding_controlled_model)

# formatted p value
tidy_fit_test_data_feeding_controlled_model$p.value <- format_p_value(tidy_fit_test_data_feeding_controlled_model$p.value)

# Changing tibble header names
tidy_fit_test_data_feeding_controlled_model <- rename_tidy_lme4_cols(tidy_fit_test_data_feeding_controlled_model)

tidy_fit_test_data_feeding_controlled_model[1:5, 4] <- test_data_feeding_controlled_model_table_row_name_vec
```

##### Results

```{r, results=TRUE, echo=FALSE}
knitr::kable(tidy_fit_test_data_feeding_controlled_model[1:5, ] %>%
  dplyr::select(-group, -effect, -component) %>%
  mutate_if(is.numeric, round, digits = 3),
caption = "Summary table for a modification of Model 3. This model is the same 
as that described in Table S1 except it includes feeding count as a covariate. 
Estimates ± standard error (SE) of the effects of trial and rewarding object 
colour of the rewarding object colour from the generalized linear mixed effect 
model containing the effects (Trial, Rewarding object colour, and their 
interaction effect)."
)
```

The main results do not change if I control for feeding count. The above table
is the output feeding controlled model. Below we have the output table for the
non-feeding-count controlled model from [Model 3](#model-3).

```{r, results=TRUE, echo=FALSE}
knitr::kable(tidy_test_data_model[1:4, ] %>%
  dplyr::select(-group, -effect, -component) %>%
  mutate_if(is.numeric, round, digits = 3))
```

In both models the p-values are similar. While the effect of feeding count is
not significant (p =
`r tidy_fit_test_data_feeding_controlled_model$'P value'[4]`) the effect of
feeding count trends in the expected direction in our feeding count controlled
model. As feeding count increases the preference for the rewarding object colour
also increases.

***

### Feeding and object preferences

This is a model I (Wyatt) ran just because I was curious. 

#### ESM Model 3 - Preference for the rewarded object during training based on feeding

This model asks whether the time spent near the rewarding object during a
training session is influenced by whether a fish ate or not. Here trial is a
variable containing the training trials (1-20). It is supplied as a random
effect along with individual ID in the model.

  - **Responsible variable:** `rewarding.object.preference` is the time
    (seconds) spent near the rewarding object subtracted by the time spent near
    the unrewarding object
  - **Fixed effect:** `ate` is a binary factor (Yes or No) and corresponds to
    whether the fish ate in a given trial
  - **Fixed effect:** `trial` is the number of the training trial. In this model
    it is supplied as an integer
  - **Random effect:** `id` is the identity of the individual fish

##### Model

```{r esm-model-1, echo=TRUE}
training_data_model_rewarding_object <-
  lmer(rewarding.object.preference ~ ate + (1 | id) + (1 | trial),
    data = training_data
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName5"> See ESM Model 3 Residuals </button>  
<div id="BlockName5" class="collapse">  

```{r, echo=FALSE, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = training_data_model_rewarding_object, n = 1000)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "ESM-model-3-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/residual-plots/",
  device = "png",
  dpi = 300
)
```

There is a significant deviation from uniformity as indicated by the significant
Kolmogorov-Smirnov test. However, this model has a particularly large sample
size (n = 456) so even slight deviations will be significant. Looking at the
effect size of the deviation (D = `r testUniformity(simulationOutput)[1]`) shows
that it is minor (D \< 0.1) and visual inspection does not suggest large
deviations in the residuals so our model is still appropriate.

</div>

</br>

```{r, echo=FALSE, results=FALSE}
# Setting table row names
training_data_model_rewarding_object_table_row_name_vec <- c(
  "Intercept",
  "Ate"
)

# Summary of esm model 3
tidy_fit_rewarding_object <- broom.mixed::tidy(training_data_model_rewarding_object)

# Formatting p value
tidy_fit_rewarding_object$p.value <- format_p_value(tidy_fit_rewarding_object$p.value)

# Changing tibble header names
tidy_fit_rewarding_object <- rename_tidy_lme4_cols(tidy_fit_rewarding_object)

tidy_fit_rewarding_object[1:2, 3] <- training_data_model_rewarding_object_table_row_name_vec
```

##### Results

```{r, results=TRUE, echo=FALSE}
knitr::kable(tidy_fit_rewarding_object[1:2, ] %>%
  dplyr::select(-group, -effect) %>%
  mutate_if(is.numeric, round, digits = 3))
```

Throughout all of training, fish that ate during a session spent on average
`r tidy_fit_rewarding_object$Estimate[2] %>% round(1)` more seconds near the
rewarded object compared to fish that did not eat (Figure,
\@ref(fig:training-data-ate-plot), p `r tidy_fit_rewarding_object$'P value'[2]`).
Fish that spent more trials eating may therefore have received more
reinforcement for the object-food association. This result is what led me to
investigate whether there was a difference in food reinforcement between the two
treatments in [Model 4](#model-4).
</br>

```{r training-data-ate-plot, echo=FALSE, message=FALSE, fig.cap="Preference for the rewarding object during training based on whether an individual ate during a trial or not. Dashed line represents the no preference value. In panel A, Data are means ± 95% CI. In panel B, the data consist of all the feeding choices of all individuals across all training trials. Data are means ± 95% CI with probability density functions to the right of the raw data.", fig.id="training-data-ate-plot", warning=FALSE}

p1 <- ggplot(
  training_data,
  aes(
    x = trial,
    y = rewarding.object.preference,
    color = ate
  )
) +
  theme_cowplot() +
  geom_jitter(width = 0.05, alpha = 0.2) +
  stat_summary(
    geom = "point",
    fun = "mean",
    size = 4.5,
    shape = 15
  ) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci", position = position_dodge(width = 0), width = 0.1
  ) +
  ylab("Rewarding object preference (sec)") +
  xlab("Trial") +
  labs(col = "Did this individual eat this trial?") +
  theme(
    legend.position = "top",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5),
    panel.border = element_rect(colour = "black", fill = NA, size = 1),
    axis.line.x = element_blank(),
    axis.line.y = element_blank()
  ) +
  scale_color_manual(values = c("#696667", "#0f85a0")) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  ylim(-310, 310)

p2 <- ggplot(
  training_data,
  aes(
    x = trial.type,
    y = rewarding.object.preference,
    color = ate
  )
) +
  theme_cowplot() +
  geom_jitter(width = 0.05, alpha = 0.1) +
  geom_flat_violin(aes(fill = ate),
    position = position_nudge(x = .25, y = 0),
    adjust = 1,
    alpha = 0.6,
    trim = TRUE,
    color = NA
  ) +
  stat_summary(
    geom = "point",
    fun = "mean",
    size = 3,
    shape = 15
  ) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci", position = position_dodge(width = 0), width = 0.15
  ) +
  xlab("") +
  ylab("Rewarding object preference (sec)") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 12.5, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5),
    axis.ticks.x = element_blank(),
    axis.line.y = element_blank(),
    axis.line.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, size = 1)
  ) +
  scale_color_manual(values = c("#696667", "#0f85a0")) +
  scale_fill_manual(values = c("#696667", "#0f85a0")) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  ylim(-310, 310) +
  scale_x_discrete(labels = "")

# Combining both plots and adding figure labels 
ggarrange(p1, p2,
  widths = c(1.5, 1),
  labels = "AUTO",
  label.x = c(0.15, 0),
  hjust = -2,
  vjust = 3,
  common.legend = TRUE
)

# Saving plot to figs folder
ggsave(
  filename = "esm-model-3-training-data-ate-plot.png",
  path = "figs/",
  device = "png",
  dpi = 300
)
```

***

## Tools used and References

A complete list of the tools used is produced below


```{r, echo=FALSE, warning=FALSE}
knitr::kable(as.data.frame(report(sessionInfo())))
```

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
